{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5MRXxhPnD8W"
   },
   "source": [
    "### Author: Rafael de Oliveira Magalhães\n",
    "\n",
    "# PEMSd3 Dataset - Cleaning Data\n",
    "\n",
    "# Download Data\n",
    "\n",
    "All data can be downloaded on the website https://pems.dot.ca.gov/.\n",
    "\n",
    "First, it is necessary to create an account to access the data.\n",
    "\n",
    "After logging in, go to 'Data Clearinghouse'. Then access:\n",
    "\n",
    "- 'Station 5-Minute' -> 'District 3'. On this page, you will find data captured by sensors from 2001 to the present moment. Each day captured by the sensors is recorded in a single .txt file. To simplify the download, it is recommended to use an extension to download multiple files automatically.\n",
    "- 'Station Metadata' -> 'District 3'. To download metadata files for the sensors, which are used to generate a map of monitored roadways.\n",
    "\n",
    "Additionally, a list with a subset of the sensors is in the PEMSd3.csv file\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwSfbwz4nD8a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, SeparableConv2D\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.metrics import Metric\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Add, Concatenate, Input, GlobalAveragePooling2D, Layer\n",
    "from keras import models, initializers\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "from spektral.datasets import TUDataset\n",
    "from spektral.layers import GCNConv, GlobalSumPool, ChebConv\n",
    "from spektral.data import SingleLoader, BatchLoader\n",
    "from spektral.data import Graph\n",
    "from spektral.data import Dataset\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, acorr_ljungbox\n",
    "from scipy.stats import f_oneway, f, kstest, norm, ks_2samp, kendalltau\n",
    "from scipy.interpolate import interp2d, RegularGridInterpolator, RectBivariateSpline, griddata\n",
    "\n",
    "# Helper libraries\n",
    "\n",
    "from bokeh.io import show\n",
    "from bokeh.plotting import gmap\n",
    "from bokeh.models import GMapOptions\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "import csv\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import folium\n",
    "import pyproj\n",
    "import math as m\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "import datetime as dt\n",
    "import re\n",
    "import time\n",
    "import gmaps as gm\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely import wkt\n",
    "from numba import jit, cuda\n",
    "from sodapy import Socrata\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings as w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4m-By6unD8f"
   },
   "source": [
    "# Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpkEKJOTnD8k"
   },
   "outputs": [],
   "source": [
    "if gpus:\n",
    "    # Alocar memória da VRAM conforme necessário\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(\"Memória da VRAM alocada conforme necessário.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKtxAGNOnD8m"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    tf.config.set_visible_devices(physical_devices[0], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oh26Ti086Z8p"
   },
   "source": [
    "# Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVQXN6-H9VCV"
   },
   "outputs": [],
   "source": [
    "def generate_dict(array: list) -> dict:\n",
    "    \"\"\"\n",
    "        Generate a dict from a array\n",
    "    \"\"\"\n",
    "    dictionary = {value: index for index, value in enumerate(array)}\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_sort(dictt):\n",
    "    return dict(sorted(dictt.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWR3RNuZ6Z8s"
   },
   "outputs": [],
   "source": [
    "def binary_search(element, array: list):\n",
    "    lo = 0\n",
    "    hi = len(array) - 1\n",
    "    while lo <= hi:\n",
    "        mid = lo + (hi - lo)//2\n",
    "        temp = array[mid]\n",
    "        if element > temp:\n",
    "            lo = mid + 1\n",
    "        elif element < temp:\n",
    "            hi = mid - 1\n",
    "        else:\n",
    "            return mid\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8RnIkcSxRWe"
   },
   "outputs": [],
   "source": [
    "def create_list_datetime(initial_date: dt.datetime, length: int) -> list:\n",
    "    \"\"\"\n",
    "        Create a list of datetime object that represents a time series\n",
    "\n",
    "        Parameters:\n",
    "        - initial_date: Initial date of the list\n",
    "        - length: Expected length of the generated list\n",
    "    \"\"\"\n",
    "    date_start = initial_date\n",
    "    array_dates = []\n",
    "    i = 0\n",
    "    while i < length:\n",
    "        array_dates.append(date_start)\n",
    "        time_change = dt.timedelta(minutes=5)\n",
    "        date_start += time_change\n",
    "        i += 1\n",
    "    return array_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r60yA3udtVD8"
   },
   "outputs": [],
   "source": [
    "def list_datetime_timedelta(initial_date: dt.datetime, final_date: dt.datetime, timedelta: int) -> list:\n",
    "    \"\"\"\n",
    "        Create a list of datetime objects by increasing time by timedelta\n",
    "    \"\"\"\n",
    "    date_start = initial_date\n",
    "    array_dates = []\n",
    "    while date_start <= final_date:\n",
    "        array_dates.append(date_start)\n",
    "        time_change = dt.timedelta(minutes=timedelta)\n",
    "        date_start += time_change\n",
    "    return array_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS185It56Z8t"
   },
   "source": [
    "# Methods - Neural Network Input Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgA1JFg46Z8u"
   },
   "outputs": [],
   "source": [
    "def data_short_time(temporal_series: np.array, list_dates: list, s: int, current_time: dt.datetime) -> tuple:\n",
    "    \"\"\"\n",
    "        Generate short time data in format of numpy array\n",
    "\n",
    "        Parameters:\n",
    "        - temporal_series: a numpy array that represents a temporal series\n",
    "        - list_dates: a list that contains the dates of the temporal series\n",
    "        - s: interval between selected samples of the temporal series.\n",
    "             This value must be an integer representing the number of time series samples to be discarded\n",
    "        - current_time: The current time selected\n",
    "\n",
    "        Returns:\n",
    "        A tuple that contains a numpy array of the short time data and the corresponding expected data\n",
    "    \"\"\"\n",
    "    dim = temporal_series.ndim\n",
    "    index_d = binary_search(current_time, list_dates)\n",
    "    pre = index_d - 1\n",
    "    if dim == 2:\n",
    "        expected_vector = temporal_series[:,index_d]\n",
    "        previous_data = temporal_series[:,(index_d-s):index_d]\n",
    "    else:\n",
    "        expected_vector = temporal_series[index_d]\n",
    "        previous_data = temporal_series[(index_d-s):index_d]\n",
    "    return (previous_data, expected_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eKlG_bM6Z8v"
   },
   "outputs": [],
   "source": [
    "def data_medium_time(temporal_series: np.array, list_dates: list, m: int, mm: int, current_time: dt.datetime) -> tuple:\n",
    "    \"\"\"\n",
    "        Generate medium time data in format of numpy array\n",
    "\n",
    "        Parameters:\n",
    "        - temporal_series: A numpy array that represents a temporal series\n",
    "        - list_dates: a list that contains the dates of the temporal series\n",
    "        - m: The number of samples in a medium time data\n",
    "        - mm: interval between selected samples of the temporal series.\n",
    "             This value must be an integer representing the number of time series samples to be discarded\n",
    "        - current_time: The current time selected\n",
    "\n",
    "        Returns:\n",
    "        A tuple that contains a numpy array of the medium time data and the corresponding expected data\n",
    "    \"\"\"\n",
    "    index_d = binary_search(current_time, list_dates)\n",
    "    dim = temporal_series.ndim\n",
    "    m *= mm\n",
    "    pre = index_d - 1\n",
    "    if dim == 2:\n",
    "        expected_vector = temporal_series[:,index_d]\n",
    "        previous_data = temporal_series[:,(index_d-m):index_d:mm]\n",
    "    else:\n",
    "        expected_vector = temporal_series[index_d]\n",
    "        previous_data = temporal_series[(index_d-m):index_d:mm]\n",
    "    return (previous_data, expected_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvfZSizr6Z8w"
   },
   "outputs": [],
   "source": [
    "def data_long_time(temporal_series: np.array, list_dates: list, l: int, ll: int, current_time: dt.datetime) -> tuple:\n",
    "    \"\"\"\n",
    "        Generate long time data in format of numpy array\n",
    "\n",
    "        Parameters:\n",
    "        - temporal_series: A numpy array that represents a temporal series\n",
    "        - list_dates: A list that contains the dates of the temporal series\n",
    "        - l: The number of samples in a long time data\n",
    "        - ll: interval between selected samples of the temporal series.\n",
    "             This value must be an integer representing the number of time series samples to be discarded\n",
    "        - current_time: The current time selected\n",
    "\n",
    "        Returns:\n",
    "        A tuple that contains a numpy array of the long time data and the corresponding expected data\n",
    "    \"\"\"\n",
    "    index_d = binary_search(current_time, list_dates)\n",
    "    dim = temporal_series.ndim\n",
    "    pre = index_d - 1\n",
    "    l *= ll\n",
    "    if dim == 2:\n",
    "        expected_vector = temporal_series[:,index_d]\n",
    "        previous_data = temporal_series[:,(index_d-l):index_d:ll]\n",
    "    else:\n",
    "        expected_vector = temporal_series[index_d]\n",
    "        previous_data = temporal_series[(index_d-l):index_d:ll]\n",
    "    return (previous_data, expected_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uCfAOTC6Z8x"
   },
   "outputs": [],
   "source": [
    "def concatenation(short_data: np.array, medium_data: np.array, long_data: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "        Concatenate short, medium and long term data\n",
    "    \"\"\"\n",
    "    if short_data.ndim == 1:\n",
    "        return np.concatenate((short_data, medium_data, long_data), axis=0)\n",
    "    return np.concatenate((short_data, medium_data, long_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhLU20_Y6Z8x"
   },
   "outputs": [],
   "source": [
    "def elements_medium_term(current_time: dt.datetime, medium_timedelta: int, sample_timedelta: int) -> int:\n",
    "    \"\"\"\n",
    "        Calculate the number of elements in a medium term\n",
    "\n",
    "        Args:\n",
    "        - current_time: The current time selected\n",
    "        - medium_timedelta: The timedelta between the initial and the final sample\n",
    "        of the medium time data\n",
    "        - sample_timedelta: The timedelta between consecutive samples of the medium time data\n",
    "    \"\"\"\n",
    "    td = dt.timedelta(hours=medium_timedelta)\n",
    "    initial_time = current_time - td\n",
    "    count = 0\n",
    "    while initial_time < current_time:\n",
    "        count += 1\n",
    "        time_change = dt.timedelta(minutes=sample_timedelta)\n",
    "        initial_time += time_change\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KN0CNQ86Z8y"
   },
   "outputs": [],
   "source": [
    "def elements_long_term(current_time: dt.datetime, long_timedelta: int, sample_timedelta: int) -> int:\n",
    "    \"\"\"\n",
    "        Calculate the number of elements in a long term\n",
    "\n",
    "        Args:\n",
    "        - current_time: The current time selected\n",
    "        - long_timedelta: The timedelta between the initial and the final sample\n",
    "        of the long time data\n",
    "        - sample_timedelta: The timedelta between consecutive samples of the long time data\n",
    "    \"\"\"\n",
    "    td = dt.timedelta(hours=long_timedelta)\n",
    "    initial_time = current_time - td\n",
    "    count = 0\n",
    "    while initial_time < current_time:\n",
    "        count += 1\n",
    "        time_change = dt.timedelta(minutes=sample_timedelta)\n",
    "        initial_time += time_change\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3XRP0oq6Z8y"
   },
   "outputs": [],
   "source": [
    "def create_Xt(temporal_series: np.array, list_dates: list, current_time: dt.datetime, medium_timedelta: int, long_timedelta: int, mm: int, ll: int) -> tuple:\n",
    "    \"\"\"\n",
    "        Create a tuple of X and y input of the GNN\n",
    "\n",
    "        Args:\n",
    "        - temporal_series: A numpy array that represents a temporal series\n",
    "        - list_dates: A list that contains the dates of the temporal series\n",
    "        - current_time: The current time selected\n",
    "        - medium_timedelta: The timedelta between the initial and the final sample\n",
    "        of the medium time data\n",
    "        - long_timedelta: The timedelta between the initial and the final sample\n",
    "        of the long time data\n",
    "        - mm: interval between selected samples of the temporal series.\n",
    "             This value must be an integer representing the number of time series samples to be discarded\n",
    "        - ll: interval between selected samples of the temporal series.\n",
    "             This value must be an integer representing the number of time series samples to be discarded\n",
    "    \"\"\"\n",
    "    delta = 5\n",
    "    s = 2\n",
    "    m = elements_medium_term(current_time, medium_timedelta, delta * mm)\n",
    "    l = elements_long_term(current_time, long_timedelta, delta * ll)\n",
    "    short_data, expected_data_1 = data_short_time(temporal_series, list_dates, s, current_time)\n",
    "    medium_data, expected_data_2 = data_medium_time(temporal_series, list_dates, m, mm, current_time)\n",
    "    long_data, expected_data_3 = data_long_time(temporal_series, list_dates, l, ll, current_time)\n",
    "    conc = concatenation(short_data, medium_data, long_data)\n",
    "    if conc.ndim == 1:\n",
    "        conc = conc.reshape(-1, 1)\n",
    "        expected_data_1 = np.expand_dims(expected_data_1, axis=0)\n",
    "    return (conc, expected_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEteFr7P6Z8z"
   },
   "outputs": [],
   "source": [
    "def initial_index(list_dates: list, long_timedelta: int) -> int:\n",
    "    \"\"\"\n",
    "        Determine the initial index to generate the data\n",
    "\n",
    "        Args:\n",
    "        - list_dates: A list that contains the dates of the temporal series\n",
    "        - long_timedelta: The timedelta between the initial and the final sample\n",
    "        of the long time data\n",
    "    \"\"\"\n",
    "    delta = dt.timedelta(hours=long_timedelta)\n",
    "    tomorrow = list_dates[0] + delta\n",
    "    now = list_dates[0]\n",
    "    count = 0\n",
    "    while now <= tomorrow:\n",
    "        count += 1\n",
    "        time_change = dt.timedelta(minutes=5)\n",
    "        now += time_change\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqOzejW68q4j"
   },
   "outputs": [],
   "source": [
    "def create_list_Xt(matrix: np.array, list_dates: list, medium_timedelta, long_timedelta, mm: int, ll: int) -> list:\n",
    "    \"\"\"\n",
    "        Create a list of input data\n",
    "    \"\"\"\n",
    "    start_index = initial_index(list_dates, long_timedelta)\n",
    "    list_Xt = []\n",
    "    for i in range(start_index,len(list_dates)):\n",
    "        tuplee = create_Xt(matrix, list_dates, list_dates[i], medium_timedelta, long_timedelta, mm, ll)\n",
    "        list_Xt.append(tuplee)\n",
    "    return list_Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-KUzQZ26Z80"
   },
   "source": [
    "# Methods - Separation of Data into Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkDZZK176Z80"
   },
   "outputs": [],
   "source": [
    "def random_split(list_Xt: list, prob_training: float, prob_validation: float) -> tuple:\n",
    "    \"\"\"\n",
    "      Split the input data in training, validation and testing sets.\n",
    "\n",
    "      Args:\n",
    "      - list_Xt: List of input data\n",
    "      - prob_training: Probability of a input being placed in training set\n",
    "      - prob_validation: Probability of a input being placed in validation set\n",
    "    \"\"\"\n",
    "    training = []\n",
    "    validation = []\n",
    "    test = []\n",
    "    for i in range(len(list_Xt)):\n",
    "        val = np.random.rand()\n",
    "        if val < prob_training:\n",
    "            training.append(list_Xt[i])\n",
    "        elif val < prob_validation:\n",
    "            validation.append(list_Xt[i])\n",
    "        else:\n",
    "            test.append(list_Xt[i])\n",
    "    return (training, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZyqtMl66Z81"
   },
   "outputs": [],
   "source": [
    "def sequential_split(list_Xt: list, frac_training: float, frac_validation: float) -> tuple:\n",
    "    \"\"\"\n",
    "      Split the input data in training, validation and testing sets.\n",
    "\n",
    "      Args:\n",
    "      - list_Xt: List of input data\n",
    "      - frac_training: Fraction of the input that will be for training\n",
    "      - frac_validation: Fraction of the input that will be for validation\n",
    "    \"\"\"\n",
    "    training = []\n",
    "    validation = []\n",
    "    test = []\n",
    "    index_training = m.floor(len(list_Xt) * frac_training)\n",
    "    index_validation = index_training + m.floor(len(list_Xt) * (frac_validation - frac_training))\n",
    "    for i in range(len(list_Xt)):\n",
    "        if i < index_training:\n",
    "            training.append(list_Xt[i])\n",
    "        elif i < index_validation:\n",
    "            validation.append(list_Xt[i])\n",
    "        else:\n",
    "            test.append(list_Xt[i])\n",
    "    return (training, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RI0ABXr_6Z81"
   },
   "outputs": [],
   "source": [
    "def split_x_and_y(list_of_tuples: list) -> tuple:\n",
    "    \"\"\"\n",
    "        Split a list of tuples into two lists\n",
    "    \"\"\"\n",
    "    sett_x = []\n",
    "    sett_y = []\n",
    "\n",
    "    for _ in range(10):\n",
    "      random.shuffle(list_of_tuples)\n",
    "\n",
    "    for tuplee in list_of_tuples:\n",
    "        x, y = tuplee\n",
    "        if x.ndim == 2:\n",
    "            lin, col = x.shape\n",
    "            ones = np.ones((lin))\n",
    "            x = np.insert(x, 0, ones, axis=1)\n",
    "        else:\n",
    "            lin = len(x)\n",
    "            ones = np.ones((1))\n",
    "            x = np.insert(x, 0, ones, axis=0)\n",
    "        sett_x.append(x)\n",
    "        sett_y.append(y)\n",
    "    return (np.array(sett_x), np.array(sett_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b-JgSnO6Z81"
   },
   "source": [
    "# Neural network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agIeMp2e6Z81"
   },
   "outputs": [],
   "source": [
    "class Dataset_C(Dataset):\n",
    "    \"\"\"\n",
    "        Create a dataset of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, adjacency_matrix: np.array, list_Xt: list, **kwargs):\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "        self.list_Xt = list_Xt\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        list_graphs = []\n",
    "        for tuplee in self.list_Xt:\n",
    "            Xt, yt = tuplee\n",
    "            list_graphs.append(Graph(x=Xt, a=self.adjacency_matrix, e=None, y=yt))\n",
    "\n",
    "        return list_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MD2_cZ-M6Z81"
   },
   "outputs": [],
   "source": [
    "class LearnableMatrixMultiplicationLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Class for learnable matrix multiplation layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int):\n",
    "        super(LearnableMatrixMultiplicationLayer, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Creates the learnable tensor with the correct dimensions\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[input_shape[-1], self.channels], trainable=True)\n",
    "        #self.kernel = self.add_weight(\"kernel\", shape=[input_shape[-1], input_shape[-2]], trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Multiply the input tensor by the learnable tensor\n",
    "        return tf.matmul(inputs, self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdMSxcng6Z83"
   },
   "outputs": [],
   "source": [
    "class FixedMatrixMultiplicationLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Class for fixed matrix multiplation layer\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super(FixedMatrixMultiplicationLayer, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Creates the learnable tensor with the correct dimensions\n",
    "        input_x, input_y = input_shape\n",
    "        #self.kernel = self.add_weight(\"kernel\", shape=[input_x[-1], input_x[-2]],initializer=initializers.Ones(),trainable=False)\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[input_x[-1], self.channels],initializer=initializers.Ones(),trainable=False)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Multiply the input tensor by the learnable tensor\n",
    "        input1, input2 = inputs\n",
    "        return tf.matmul(input1,self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BL3pevtB6Z83"
   },
   "outputs": [],
   "source": [
    "def rmse(y_true: np.array, y_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error\n",
    "    Args:\n",
    "        y_true ([np.array]): test samples\n",
    "        y_pred ([np.array]): predicted samples\n",
    "    Returns:\n",
    "        [float]: root mean squared error\n",
    "    \"\"\"\n",
    "    if y_pred.ndim == 3:\n",
    "        y_pred = tf.squeeze(y_pred, axis=-1)\n",
    "    y_pred2 = tf.squeeze(y_pred)\n",
    "    return K.sqrt(K.mean(K.square(y_pred2 - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xws0mm7S6Z83"
   },
   "outputs": [],
   "source": [
    "def nrmse(y_true: np.array, y_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Normalized Root Mean Squared Error\n",
    "    Args:\n",
    "        y_true ([np.array]): test samples\n",
    "        y_pred ([np.array]): predicted samples\n",
    "    Returns:\n",
    "        [float]: normalized root mean squared error\n",
    "    \"\"\"\n",
    "    if y_pred.ndim == 3:\n",
    "        y_pred = tf.squeeze(y_pred, axis=-1)\n",
    "    y_pred2 = tf.squeeze(y_pred)\n",
    "    return K.sqrt(K.mean(K.square(y_pred2 - y_true), axis=-1)) / K.mean(K.abs(y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error\n",
    "    \"\"\"\n",
    "    y_true_tensor = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred_tensor = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "    return K.sqrt(K.mean(K.square(y_pred_tensor - y_true_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrmse2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Normalized Root Mean Squared Error\n",
    "    \"\"\"\n",
    "    y_true_tensor = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred_tensor = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    return K.sqrt(K.mean(K.square(y_pred_tensor - y_true_tensor))) / K.mean(y_true_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-3PjfkB6Z84"
   },
   "outputs": [],
   "source": [
    "def squared_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "        Method for square error loss\n",
    "    \"\"\"\n",
    "    error = tf.square(y_true - y_pred)\n",
    "    loss = tf.reduce_mean(error)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHk2xHpz6Z84"
   },
   "outputs": [],
   "source": [
    "class GNN(Model):\n",
    "    \"\"\"\n",
    "        Class for GNN model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, k_layers: int, relu_last=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.k_layers = k_layers\n",
    "        self.num_layers = len(channels)\n",
    "        self.relu_last = relu_last\n",
    "        self.init_layers()\n",
    "\n",
    "    def init_layers(self):\n",
    "        self.concatenate = Concatenate(axis=2)\n",
    "        self.add = Add()\n",
    "        self.relu = tf.keras.layers.Activation('relu')\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            setattr(self, f'cheb_stgi_l{i+1}', [])\n",
    "            kk = self.k_layers[i]\n",
    "            for k in range(1, kk + 1):\n",
    "                layer = ChebConv(self.channels[i], K=k, activation='relu', use_bias=True)\n",
    "                getattr(self, f'cheb_stgi_l{i+1}').append(layer)\n",
    "\n",
    "        self.dot_learnable_layers = [LearnableMatrixMultiplicationLayer(self.channels[i]) for i in range(self.num_layers)]\n",
    "        self.dot_fixed_layers = [FixedMatrixMultiplicationLayer(self.channels[i]) for i in range(self.num_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs\n",
    "        out = None\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            cheb_stgi_layers = getattr(self, f'cheb_stgi_l{i+1}')\n",
    "            out_layers = [cheb(inputs) for cheb in cheb_stgi_layers]\n",
    "            concatenate = self.concatenate(out_layers)\n",
    "            mult_learnable = self.dot_learnable_layers[i](concatenate)\n",
    "            mult_fixed = self.dot_fixed_layers[i](inputs)\n",
    "            add = self.add([mult_learnable, mult_fixed])\n",
    "            if i < self.num_layers - 1 or self.relu_last:\n",
    "                relu = self.relu(add)\n",
    "                out = relu\n",
    "            else:\n",
    "                out = add\n",
    "            inputs = (out,y)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kBPlazF6Z84"
   },
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09H1mMBuowED"
   },
   "outputs": [],
   "source": [
    "def clean_txt(archive: str, directory_initial: str, directory_destiny: str, sensors, specific_sensor=None) -> None:\n",
    "    \"\"\"\n",
    "        Cleans texts files by deleting unnecessary columns\n",
    "    \"\"\"\n",
    "    new_name = directory_destiny + \"/\" + archive[:-3] + \"csv\"\n",
    "    og_name = directory_initial + \"/\" + archive\n",
    "    with open(new_name, 'w+') as new_arc:\n",
    "        with open(og_name, \"r\") as arc:\n",
    "            first_line = 'Timestamp,Station,Flow,Speed\\n'\n",
    "            new_arc.write(first_line)\n",
    "            for line in arc:\n",
    "                content = line.split(\",\")\n",
    "                try:\n",
    "                    sensor = int(content[1])\n",
    "                except:\n",
    "                    continue\n",
    "                if specific_sensor is not None:\n",
    "                    if sensor == specific_sensor:\n",
    "                        new_line = content[0] + \",\" + content[1] + \",\" + content[9] + \",\" + content[11] + \"\\n\"\n",
    "                        new_arc.write(new_line)\n",
    "                else:\n",
    "                    if sensor in sensors:\n",
    "                        new_line = content[0] + \",\" + content[1] + \",\" + content[9] + \",\" + content[11] + \"\\n\"\n",
    "                        new_arc.write(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPXGmKqCtN2M"
   },
   "outputs": [],
   "source": [
    "def through_directory(directory_initial: str, directory_csv_files: str, sensors, specific_sensor=None) -> None:\n",
    "    \"\"\"\n",
    "        Cleans text files from a directory and creates csv files\n",
    "    \"\"\"\n",
    "    cont = 0\n",
    "    for name_archive in os.listdir(directory_initial):\n",
    "        print(cont)\n",
    "        if name_archive.endswith('.txt'):\n",
    "            clean_txt(name_archive, directory_initial, directory_csv_files, sensors, specific_sensor)\n",
    "            cont += 1\n",
    "            #generate_csv(name_archive, directory_txt_files, directory_csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VBye9gb6Z85",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_sensors = generate_dict_sensors(\"PEMSd3.csv\")\n",
    "list_sensors = set(dict_sensors.keys())\n",
    "directory_initial = \"/home/rafael_o_magalhaes/Documentos/Python Projects/PEMS2\"\n",
    "directory_txt_files = \"/home/rafael_o_magalhaes/Documentos/Python Projects/pemsd7_2023 - clean\"\n",
    "directory_csv_files = \"/home/rafael_o_magalhaes/Documentos/Python Projects/PEMS2 - csv2\"\n",
    "through_directory(directory_initial, directory_csv_files, list_sensors, specific_sensor=317842)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1cwNwP06Z86"
   },
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jet4EWWPu-0"
   },
   "source": [
    "### Methods - Select Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HF9r_d_vd01"
   },
   "outputs": [],
   "source": [
    "def generate_dict_sensors(archive: str) -> dict:\n",
    "    \"\"\"\n",
    "      Read a csv file that contains the sensors and create a sensors dictionary\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(archive).dropna()\n",
    "    unique_stations = pd.unique(dataset['from'])\n",
    "    return generate_dict(unique_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCAzxNVH6Z86"
   },
   "outputs": [],
   "source": [
    "def exclude_sensors(dict_sensors: dict, exclude_lines: list) -> dict:\n",
    "    \"\"\"\n",
    "        Exclude invalid sensors from the sensors dictionary\n",
    "    \"\"\"\n",
    "    new_sensors = []\n",
    "    sensors = list(dict_sensors.keys())\n",
    "    for i in range(len(sensors)):\n",
    "        if i not in exclude_lines:\n",
    "            new_sensors.append(sensors[i])\n",
    "    return generate_dict(new_sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4B9biWTP8WF"
   },
   "source": [
    "### Methods - Generate Temporal Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJHK6tSU49O7"
   },
   "outputs": [],
   "source": [
    "def insert_flow_matrix(dict_dates: dict, dict_sensors: dict, directory: str) -> np.array:\n",
    "  \"\"\"\n",
    "      Insert flow data in temporal series\n",
    "  \"\"\"\n",
    "  matrix = np.full((len(dict_sensors), len(dict_dates)), np.nan)\n",
    "  for name_archive in os.listdir(directory):\n",
    "    if name_archive.endswith('.csv'):\n",
    "        print(name_archive)\n",
    "        archive = directory + \"/\" + name_archive\n",
    "        dataset = pd.read_csv(archive)\n",
    "        for i in range(len(dataset)):\n",
    "          line = dataset.iloc[i]\n",
    "          time = line['Timestamp']\n",
    "          sensor = line['Station']\n",
    "          try:\n",
    "            ii = dict_sensors[sensor]\n",
    "          except:\n",
    "            continue\n",
    "          format = \"%m/%d/%Y %H:%M:%S\"\n",
    "          date = dt.datetime.strptime(time, format)\n",
    "          j = dict_dates[date]\n",
    "          matrix[ii][j] = line['Flow']\n",
    "  return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_flow_list(dict_dates: dict, directory: str) -> np.array:\n",
    "  \"\"\"\n",
    "      Insert flow data in temporal series\n",
    "  \"\"\"\n",
    "  matrix = np.full((len(dict_dates)), np.nan)\n",
    "  for name_archive in os.listdir(directory):\n",
    "    if name_archive.endswith('.csv'):\n",
    "        print(name_archive)\n",
    "        archive = directory + \"/\" + name_archive\n",
    "        dataset = pd.read_csv(archive)\n",
    "        for i in range(len(dataset)):\n",
    "          line = dataset.iloc[i]\n",
    "          time = line['Timestamp']\n",
    "          format = \"%m/%d/%Y %H:%M:%S\"\n",
    "          date = dt.datetime.strptime(time, format)\n",
    "          j = dict_dates[date]\n",
    "          matrix[j] = line['Flow']\n",
    "  return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_count_nan(directory: str) -> dict:\n",
    "    dictt = {}\n",
    "    cont = 0\n",
    "    for name_archive in os.listdir(directory):\n",
    "        if name_archive.endswith('.csv'):\n",
    "            print(cont)\n",
    "            cont += 1\n",
    "            archive = directory + \"/\" + name_archive\n",
    "            dataset = pd.read_csv(archive)\n",
    "            for i in range(len(dataset)):\n",
    "                line = dataset.iloc[i]\n",
    "                flow = line['Flow']\n",
    "                sensor = line['Station']\n",
    "                if np.isnan(flow):\n",
    "                    if sensor not in dictt:\n",
    "                        dictt[sensor] = 1\n",
    "                    else:\n",
    "                        dictt[sensor] += 1\n",
    "    return dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory_csv_files = \"/home/rafael_o_magalhaes/Documentos/Python Projects/PEMS2 - csv\"\n",
    "dict_nan_sensors = dict_count_nan(directory_csv_files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory_csv_files = \"/home/rafael_o_magalhaes/Documentos/Python Projects/PEMS2 - csv2\"\n",
    "initial_date = dt.datetime(2001, 1, 1, 0, 0, 0)\n",
    "final_date = dt.datetime(2023, 12, 31, 23, 55, 0)\n",
    "list_datetime = list_datetime_timedelta(initial_date, final_date, 5)\n",
    "dict_dates = generate_dict(list_datetime)\n",
    "list_flow = insert_flow_list(dict_dates, directory_csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"list_pemsd3.npy\", list_flow) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y07068z-O4e_"
   },
   "source": [
    "### Methods - Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRNnf-W56Z87"
   },
   "outputs": [],
   "source": [
    "def avaliate_nan_values(matrix: np.array) -> tuple:\n",
    "    \"\"\"\n",
    "        Evaluate the amount of NaN values, of non NaN values, the percentage of NaN values and the list of NaN lines\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "    nan_lines = pd.DataFrame(columns=['NaN Values', 'Percentage'])\n",
    "    count_nan = 0\n",
    "    total = lin * col\n",
    "    for i in range(lin):\n",
    "        line = matrix[i]\n",
    "        known_indexes = np.arange(len(line))[~np.isnan(line)]\n",
    "        if len(known_indexes) == 0:\n",
    "            nan_lines.loc[i] = [m.inf, m.inf]\n",
    "            total -= col\n",
    "            continue\n",
    "        unknown_indexes = np.arange(len(line))[np.isnan(line)]\n",
    "        count_nan += len(unknown_indexes)\n",
    "        nan_lines.loc[i] = [len(unknown_indexes), len(unknown_indexes)/col * 100]\n",
    "    return (count_nan, total, count_nan/total * 100, nan_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliate_nan_values_list(matrix: np.array) -> tuple:\n",
    "    \"\"\"\n",
    "        Evaluate the amount of NaN values, of non NaN values, the percentage of NaN values and the list of NaN lines\n",
    "    \"\"\"\n",
    "    length = len(matrix)\n",
    "    nan_lines = pd.DataFrame(columns=['NaN Values', 'Percentage'])\n",
    "    known_indexes = np.arange(length)[~np.isnan(matrix)]\n",
    "    if len(known_indexes) == 0:\n",
    "        nan_lines.loc[0] = [m.inf, m.inf]\n",
    "    else:\n",
    "        unknown_indexes = np.arange(length)[np.isnan(matrix)]\n",
    "        nan_lines.loc[0] = [len(unknown_indexes), len(unknown_indexes)/length * 100]\n",
    "    return nan_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_list(matrix: np.array) -> tuple:\n",
    "    \"\"\"\n",
    "        Interpolate a numpy array to fill the NaN values and exclude the NaN lines\n",
    "    \"\"\"\n",
    "    length = len(matrix)\n",
    "    known_indexes = np.arange(length)[~np.isnan(matrix)] \n",
    "    # Find the index of null values (NaN)\n",
    "    unknown_indexes = np.arange(length)[np.isnan(matrix)]\n",
    "    # Use the interp function to calculate estimated values for NaN\n",
    "    estimated_values = np.interp(unknown_indexes, known_indexes, matrix[~np.isnan(matrix)])\n",
    "    # Replace NaN values with estimated values\n",
    "    matrix[unknown_indexes] = estimated_values\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpWTRkEn6Z88"
   },
   "outputs": [],
   "source": [
    "def interpolate_matrix(matrix: np.array) -> tuple:\n",
    "    \"\"\"\n",
    "        Interpolate a numpy array to fill the NaN values and exclude the NaN lines\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "    exclude_lines = []\n",
    "    for i in range(lin):\n",
    "        line = matrix[i]\n",
    "        known_indexes = np.arange(len(line))[~np.isnan(line)]\n",
    "        if len(known_indexes) == 0:\n",
    "            exclude_lines.append(i)\n",
    "            continue\n",
    "        # Find the index of null values (NaN)\n",
    "        unknown_indexes = np.arange(len(line))[np.isnan(line)]\n",
    "        if len(unknown_indexes) == 0:\n",
    "            continue\n",
    "        # Use the interp function to calculate estimated values for NaN\n",
    "        estimated_values = np.interp(unknown_indexes, known_indexes, line[~np.isnan(line)])\n",
    "        # Replace NaN values with estimated values\n",
    "        line[unknown_indexes] = estimated_values\n",
    "        matrix[i] = line\n",
    "    return (matrix, exclude_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_polynomial(arr):\n",
    "    mask = np.isnan(arr)\n",
    "    x = np.flatnonzero(~mask)\n",
    "    y = arr[~mask]\n",
    "    p = np.poly1d(np.polyfit(x, y, deg=min(2, len(x)-1)))\n",
    "    arr[mask] = p(np.flatnonzero(mask))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polinomial_interpolation(matrix: np.array) -> tuple:\n",
    "    return np.apply_along_axis(fill_nan_polynomial, axis=1, arr=matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrange_interpolation(x, y, xi):\n",
    "    yi = 0\n",
    "    for i in range(len(x)):\n",
    "        numerator, denominator = 1, 1\n",
    "        for j in range(len(x)):\n",
    "            if i != j:\n",
    "                numerator *= xi - x[j]\n",
    "                denominator *= x[i] - x[j]\n",
    "        yi += y[i] * (numerator / denominator)\n",
    "    return yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrange_interpolate_row(row):\n",
    "    nan_indices = np.isnan(row)\n",
    "    x_known = np.flatnonzero(~nan_indices)\n",
    "    y_known = row[~nan_indices]\n",
    "    row[nan_indices] = [lagrange_interpolation(x_known, y_known, xi) for xi in np.flatnonzero(nan_indices)]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrange_interpolate_matrix(matrix):\n",
    "    return np.apply_along_axis(lagrange_interpolate_row, axis=1, arr=matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divided_differences(x, y):\n",
    "    n = len(x)\n",
    "    F = np.zeros((n, n))\n",
    "    F[:, 0] = y\n",
    "\n",
    "    for j in range(1, n):\n",
    "        for i in range(n - j):\n",
    "            F[i, j] = (F[i + 1, j - 1] - F[i, j - 1]) / (x[i + j] - x[i])\n",
    "\n",
    "    return F[0, :]\n",
    "\n",
    "def newton_interpolation(x, y, xi):\n",
    "    n = len(x)\n",
    "    F = divided_differences(x, y)\n",
    "    p = F[0]\n",
    "    for j in range(1, n):\n",
    "        p += F[j] * np.prod(xi - x[:j])\n",
    "    return p\n",
    "\n",
    "def newton_interpolate_row(row):\n",
    "    nan_indices = np.isnan(row)\n",
    "    x_known = np.flatnonzero(~nan_indices)\n",
    "    y_known = row[~nan_indices]\n",
    "    row[nan_indices] = [newton_interpolation(x_known, y_known, xi) for xi in np.flatnonzero(nan_indices)]\n",
    "    return row\n",
    "\n",
    "def newton_interpolate_matrix(matrix):\n",
    "    return np.apply_along_axis(newton_interpolate_row, axis=1, arr=matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spline_interpolate_matrix(matrix):\n",
    "    nan_indices = np.isnan(matrix)\n",
    "    row, col = np.indices(matrix.shape)\n",
    "    x_known = col[~nan_indices]\n",
    "    y_known = row[~nan_indices]\n",
    "    values_known = matrix[~nan_indices]\n",
    "    \n",
    "    points = np.transpose(np.nonzero(nan_indices))\n",
    "    interpolated_values = griddata((x_known, y_known), values_known, (points[:, 1], points[:, 0]), method='cubic')\n",
    "    \n",
    "    matrix[nan_indices] = interpolated_values\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic_interpolate_matrix(matrix):\n",
    "    nan_indices = np.isnan(matrix)\n",
    "    y, x = np.mgrid[:matrix.shape[0], :matrix.shape[1]]\n",
    "    x_known = x[~nan_indices]\n",
    "    y_known = y[~nan_indices]\n",
    "    values_known = matrix[~nan_indices]\n",
    "\n",
    "    f = RegularGridInterpolator((y_known, x_known), values_known)\n",
    "\n",
    "    points = np.transpose(np.nonzero(nan_indices))\n",
    "    interpolated_values = f(points)\n",
    "\n",
    "    matrix[nan_indices] = interpolated_values\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtDcsTWaPHZy"
   },
   "source": [
    "### Methods - Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxmx6mArCsRR"
   },
   "outputs": [],
   "source": [
    "def generate_transition_matrix(dict_sensors,exclude_lines):\n",
    "  matrix = np.zeros((len(dict_sensors),len(dict_sensors)))\n",
    "  graph_csv = pd.read_csv(\"PEMSd3.csv\")\n",
    "  graph_csv = graph_csv.dropna()\n",
    "  for i in range(len(graph_csv)):\n",
    "    line = graph_csv.iloc[i]\n",
    "    try:\n",
    "        ii = dict_sensors[int(line['from'])]\n",
    "        j = dict_sensors[int(line['to'])]\n",
    "    except:\n",
    "        continue\n",
    "    matrix[ii][j] = line['distance']\n",
    "  return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmNsjuV2GBbQ"
   },
   "outputs": [],
   "source": [
    "def max_avg(dict_sensors: dict, directory_initial: str) -> tuple:\n",
    "  list_max = np.zeros(len(dict_sensors))\n",
    "  list_count = np.zeros(len(dict_sensors))\n",
    "\n",
    "  for i in range(len(list_max)):\n",
    "    list_max[i] = -m.inf\n",
    "\n",
    "  for name_archive in os.listdir(directory_initial):\n",
    "    if name_archive.endswith('.csv'):\n",
    "      print(name_archive)\n",
    "      archive = directory_initial + \"/\" + name_archive\n",
    "      dataset = pd.read_csv(archive)\n",
    "      dataset = dataset.dropna(subset=['Speed'])\n",
    "      for i in range(len(dataset)):\n",
    "        line = dataset.iloc[i]\n",
    "        sensor = line['Station']\n",
    "        speed = line['Speed']\n",
    "        try:\n",
    "            index = dict_sensors[sensor]\n",
    "        except:\n",
    "            continue\n",
    "        list_count[index] += 1\n",
    "        if speed > list_max[index]:\n",
    "          list_max[index] = speed\n",
    "  return (list_max, list_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XNuL5GnMDqA"
   },
   "outputs": [],
   "source": [
    "def avg_speed(dict_sensors: dict, list_count: list, directory_initial: str) -> np.array:\n",
    "  \"\"\"\n",
    "      Generate a numpy array of average speed for each node\n",
    "  \"\"\"\n",
    "  list_avg = np.zeros(len(dict_sensors), dtype=float)\n",
    "  for name_archive in os.listdir(directory_initial):\n",
    "    if name_archive.endswith('.csv'):\n",
    "      print(name_archive)\n",
    "      archive = directory_initial + \"/\" + name_archive\n",
    "      dataset = pd.read_csv(archive)\n",
    "      dataset = dataset.dropna(subset=['Speed'])\n",
    "      for i in range(len(dataset)):\n",
    "        line = dataset.iloc[i]\n",
    "        sensor = line['Station']\n",
    "        speed = line['Speed']\n",
    "        try:\n",
    "            index = dict_sensors[sensor]\n",
    "        except:\n",
    "            continue\n",
    "        list_avg[index] += speed / list_count[index]\n",
    "  return list_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMCQem7W6Z8-"
   },
   "outputs": [],
   "source": [
    "def interpolate_values(listt: np.array, value) -> np.array:\n",
    "    \"\"\"\n",
    "      Replace values equals 'value' by the mean of the other values\n",
    "    \"\"\"\n",
    "    sum_values = 0.0\n",
    "    count = 0\n",
    "    list_index = []\n",
    "    for i in range(len(listt)):\n",
    "        elem = listt[i]\n",
    "        if elem != value:\n",
    "            sum_values += elem\n",
    "            count += 1\n",
    "        else:\n",
    "            list_index.append(i)\n",
    "    for elem in list_index:\n",
    "        listt[elem] = sum_values/count\n",
    "    return listt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5t8LwAk6Z8-"
   },
   "outputs": [],
   "source": [
    "def transition_matrix_definitive(matrix_og: np.array, exclude_lines: list) -> np.array:\n",
    "    \"\"\"\n",
    "        Exclude sensors (lines and columns) from the transition/adjacency matrix\n",
    "    \"\"\"\n",
    "    lin, col = matrix_og.shape\n",
    "    for i in range(lin):\n",
    "        for j in range(len(exclude_lines)):\n",
    "            jj = exclude_lines[j]\n",
    "            elem = matrix_og[i][jj]\n",
    "            if elem != 0.0:\n",
    "                for k in range(col):\n",
    "                    val = matrix_og[jj][k]\n",
    "                    if val != 0.0 and matrix_og[i][k] == 0.0:\n",
    "                        matrix_og[i][k] = val\n",
    "    matrix_og = np.delete(matrix_og, exclude_lines, axis=0)\n",
    "    matrix_og = np.delete(matrix_og, exclude_lines, axis=1)\n",
    "    return matrix_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUranbqzN31e"
   },
   "outputs": [],
   "source": [
    "def definitive_transition_matrix(matrix: np.array, list_max: np.array, list_avg: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "        Fill the transition matrix\n",
    "\n",
    "        Args:\n",
    "        - matrix: The transition matrix\n",
    "        - list_max: The list of max speed for each node\n",
    "        - list_avg: The list of average speed for each node\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "    print(lin, col)\n",
    "    print(len(list_max), len(list_avg))\n",
    "    for i in range(lin):\n",
    "        count = 0\n",
    "        for j in range(col):\n",
    "            if matrix[i][j] != 0.0 and i != j:\n",
    "                count += 1\n",
    "\n",
    "        if count == 0:\n",
    "            matrix[i][i] = 1\n",
    "            print(i)\n",
    "            continue\n",
    "\n",
    "        matrix[i][i] = (list_max[i] - list_avg[i])/list_avg[i]\n",
    "\n",
    "        for j in range(col):\n",
    "            if matrix[i][j] != 0.0 and i != j:\n",
    "                matrix[i][j] = (1 - matrix[i][i])/count\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnKbaPfCxRW7"
   },
   "source": [
    "# Methods - Data Temporal Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayAOuf54xRW7"
   },
   "outputs": [],
   "source": [
    "def remove_data(matrix: np.array, interval: int) -> np.array:\n",
    "    \"\"\"\n",
    "        Removes columns from a temporal series\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "    list_index = np.zeros(col) == 1\n",
    "    i = 0\n",
    "    while i < col:\n",
    "        list_index[i] = True\n",
    "        i += interval\n",
    "    return matrix[:,list_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5ym53Q7xRW8"
   },
   "source": [
    "# Methods - Removing Data for Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zON4dFOxRW8"
   },
   "outputs": [],
   "source": [
    "def create_nan_columns(matrix: np.array, interval: int) -> np.array:\n",
    "    \"\"\"\n",
    "        Creates NaN columns into a temporal series\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "    nan_column = np.full((lin), np.nan)\n",
    "    i = 0\n",
    "    while i < col:\n",
    "        if (i % (interval + 1) != 0):\n",
    "            matrix[:, i] = nan_column\n",
    "        i += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjLmTgzpxRW9"
   },
   "outputs": [],
   "source": [
    "def remove_random_data(matrix: np.array, probability: float, length) -> np.array:\n",
    "    \"\"\"\n",
    "        Replaces values to NaN\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "    for i in range(lin):\n",
    "        for j in range(length):\n",
    "            prob = random.random()\n",
    "            if prob < probability:\n",
    "                matrix[i][j] = np.nan\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVDQvwcpxRW-"
   },
   "source": [
    "# Methods - Sensor Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhXX__3oxRW-"
   },
   "outputs": [],
   "source": [
    "def random_index(length: int, probability: float) -> np.array:\n",
    "    \"\"\"\n",
    "        Creates a boolean array of sensors that will be excluded\n",
    "    \"\"\"\n",
    "    array = np.ones((length), dtype=int)\n",
    "    for i in range(length):\n",
    "        prob = random.random()\n",
    "        if prob < probability:\n",
    "            array[i] = 0\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyYGavk6xRW-"
   },
   "outputs": [],
   "source": [
    "def remove_sensors(matrix: np.array, list_index: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "        Remove sensors from a adjacency matrix\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "\n",
    "    for i in range(len(list_index)):\n",
    "        if list_index[i] == 0:\n",
    "            adj_i = []\n",
    "            for j in range(col):\n",
    "                if matrix[i][j] != 0.0:\n",
    "                    adj_i.append(j)\n",
    "\n",
    "            for j in range(lin):\n",
    "                if matrix[j][i] != 0.0:\n",
    "                    for k in adj_i:\n",
    "                        matrix[j][k] = 1\n",
    "\n",
    "    boolean_array = list_index == 1\n",
    "    matrix = matrix[:,boolean_array]\n",
    "    return matrix[boolean_array,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHbepmXWxRW_"
   },
   "outputs": [],
   "source": [
    "def update_data_matrix(matrix: np.array, list_index: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "        Remove sensors from the temporal series\n",
    "    \"\"\"\n",
    "    list_index2 = []\n",
    "    for i in range(len(list_index)):\n",
    "        if list_index[i] == 0:\n",
    "            list_index2.append(i)\n",
    "    return np.delete(matrix, list_index2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmmvO0W4lLF7"
   },
   "outputs": [],
   "source": [
    "def remove_sensors_list(listt: np.array, list_index: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "        Remove sensors from a list\n",
    "    \"\"\"\n",
    "    boolean_array = list_index == 1\n",
    "    return listt[boolean_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods - Sensor Sparsity - Stretch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_zeros(transition_matrix, list_index):\n",
    "    count_list = []\n",
    "    for i in range(len(list_index)):\n",
    "        line = transition_matrix[list_index[i]]\n",
    "        count_list.append(np.count_nonzero(line))\n",
    "    return count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbors(transition_matrix, list_index):\n",
    "    list_neighbors = []\n",
    "    size = len(list_index)\n",
    "    for i in range(size):\n",
    "        line = transition_matrix[list_index[i]]\n",
    "        listt = []\n",
    "        for j in range(len(line)):\n",
    "            if line[j] != 0.0:\n",
    "                if i != size - 1:\n",
    "                    if list_index[i+1] != j:\n",
    "                        listt.append(j)\n",
    "                else:\n",
    "                    listt.append(j)\n",
    "        list_neighbors.append(listt)\n",
    "    return list_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def origins(transition_matrix, list_index):\n",
    "    list_origins = []\n",
    "    size = len(list_index)\n",
    "    for i in range(size):\n",
    "        line = transition_matrix[:,list_index[i]]\n",
    "        listt = []\n",
    "        for j in range(len(line)):\n",
    "            if line[j] != 0.0:\n",
    "                if i > 0:\n",
    "                    if list_index[i-1] != j:\n",
    "                        listt.append(j)\n",
    "                else:\n",
    "                    listt.append(j)\n",
    "        list_origins.append(listt)\n",
    "    return list_origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbors_origins(transition_matrix, list_index):\n",
    "    list_origins = []\n",
    "    list_neighbors = []\n",
    "    size = len(list_index)\n",
    "    for i in range(size):\n",
    "        line = transition_matrix[list_index[i]]\n",
    "        column = transition_matrix[:,list_index[i]]\n",
    "        length = len(line)\n",
    "        listt_origins = []\n",
    "        listt_neighbors = []\n",
    "        for j in range(length):\n",
    "            if column[j] != 0.0:\n",
    "                listt_origins.append(j)\n",
    "            if line[j] != 0.0:\n",
    "                listt_neighbors.append(j)\n",
    "        list_origins.append(listt_origins)\n",
    "        list_neighbors.append(listt_neighbors)\n",
    "    return (list_origins, list_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanse_boolean_array(size, list_index, list_bool, same_position):\n",
    "    boolean_array = np.zeros(size, dtype=int) == 0\n",
    "    for i in range(len(list_index)):\n",
    "        if not list_bool[i]:\n",
    "            boolean_array[list_index[i]] = False\n",
    "    for i in range(len(list_bool)):\n",
    "        if not list_bool[i]:\n",
    "            array_same = same_position[i]\n",
    "            for index in array_same:\n",
    "                boolean_array[index] = False\n",
    "    return boolean_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_sensors(np_array, list_bool):\n",
    "    np_array2 = np.delete(np_array, np.where(list_bool), axis=0)\n",
    "    return np.delete(np_array2, np.where(list_bool), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbors_list(path, list_neighbors, list_origins):\n",
    "    length = len(path) - 1\n",
    "    j = 0\n",
    "    k = 1\n",
    "    same_position = [[]]\n",
    "    dict_input = {}\n",
    "    dict_output = {}\n",
    "    list_nodes = [0]\n",
    "    for i in range(1, length):\n",
    "        neighbors = list_neighbors[i-1]\n",
    "        origins = list_origins[i+1]\n",
    "        if neighbors == origins:\n",
    "            same_position.append(neighbors)\n",
    "        elif len(neighbors) > len(origins):\n",
    "            list_nodes.append(i)\n",
    "            neig = set(neighbors)\n",
    "            og = set(origins)\n",
    "            same_position.append(list(neig & og))\n",
    "            diff = list(neig - og)\n",
    "            dict_input[path[i]] = diff\n",
    "        else:\n",
    "            list_nodes.append(i)\n",
    "            neig = set(neighbors)\n",
    "            og = set(origins)\n",
    "            same_position.append(list(neig & og))\n",
    "            diff = list(og - neig)\n",
    "            dict_output[path[i]] = diff\n",
    "        j += 1\n",
    "        k += 1\n",
    "    list_nodes.append(length)\n",
    "    same_position.append([])\n",
    "    return (same_position, dict_input, dict_output, list_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_sensors_path(list_bool, interval: int, start: int, end: int) -> np.array:\n",
    "    i = start\n",
    "    while i < end:\n",
    "        list_bool[i] = True\n",
    "        i += interval\n",
    "    return list_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_path(path, interval, list_index):\n",
    "    length = len(path)\n",
    "    list_bool = np.zeros(length) == 1\n",
    "    for i in range(len(list_index) - 1):\n",
    "        list_bool = delete_sensors_path(list_bool, interval, list_index[i], list_index[i+1])\n",
    "    list_bool[length - 1] = True\n",
    "    return list_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_subpath(path: np.array) -> np.array:\n",
    "    list_index = np.zeros(len(path)) == 1\n",
    "    return list_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_bool(list_bool):\n",
    "    array = []\n",
    "    for i in range(len(list_bool)):\n",
    "        if list_bool[i]:\n",
    "            array.append(i)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_position_array(origins):\n",
    "    new = origins[1:]\n",
    "    return new + [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_distances(transition_matrix, path, boolean_array):\n",
    "    dict_distance = {}\n",
    "    sum = 0.0\n",
    "    initial_index = 0\n",
    "    for i in range(1, len(boolean_array)):\n",
    "        preview = path[i-1]\n",
    "        current = path[i]\n",
    "        sum += transition_matrix[preview][current]\n",
    "        if boolean_array[i]:\n",
    "            dict_distance[(initial_index, i)] = sum\n",
    "            initial_index = i\n",
    "    return dict_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def exclude(transition_matrix, list_index, list_bool, same_position, dict_distance):\n",
    "    array_index = index_bool(list_bool)\n",
    "    for i in range(len(array_index)):\n",
    "        if i != len(array_index) - 1:\n",
    "            current_index = list_index[array_index[i]]\n",
    "            next_index = list_index[array_index[i + 1]]\n",
    "            list_nodes = same_position[array_index[i]] + [current_index]\n",
    "            list_neighbors = same_position[array_index[i+1]] + [next_index]\n",
    "            for current in list_nodes:\n",
    "                for next in list_neighbors:\n",
    "                    transition_matrix[current][next] = dict_distance[(array_index[i], array_index[i+1])]\n",
    "    list_bool_full = expanse_boolean_array(len(transition_matrix), list_index, list_bool, same_position)\n",
    "    transition_matrix = transition_matrix[:,list_bool_full]\n",
    "    return transition_matrix[list_bool_full,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_noise_matrix(temporal_series, noise_matrix):\n",
    "    result_matrix = temporal_series + noise_matrix\n",
    "    lines, columns = temporal_series.shape\n",
    "    for i in range(lines):\n",
    "        for j in range(columns):\n",
    "            if result_matrix[i][j] < 0.0:\n",
    "                result_matrix[i][j] = 0.0\n",
    "    return result_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7B0Tuyh6Z9F"
   },
   "source": [
    "## Generate Transition Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX-kWsFglLF8"
   },
   "source": [
    "### Load sensors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SD7dDeIe6Z9F"
   },
   "outputs": [],
   "source": [
    "# Sensors without data\n",
    "exclude_lines = [4, 78, 85, 198, 260, 316, 330, 331, 332, 333, 334, 338, 339]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWlaeXAV6Z9F"
   },
   "outputs": [],
   "source": [
    "dict_sensors = generate_dict_sensors(\"PEMSd3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3jjoAAq6Z9F"
   },
   "outputs": [],
   "source": [
    "transition_matrix = generate_transition_matrix(dict_sensors, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3iP7oJzC6Z9G"
   },
   "outputs": [],
   "source": [
    "transition_matrix = transition_matrix_definitive(transition_matrix, exclude_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for sensors sparsity testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grafo:\n",
    "    def __init__(self, matriz):\n",
    "        self.matriz = matriz\n",
    "\n",
    "    def dfs_util(self, u, destino, visitado, caminho):\n",
    "        visitado[u] = True\n",
    "        caminho.append(u)\n",
    "\n",
    "        if u == destino:\n",
    "            print(\"Caminho encontrado:\", caminho)\n",
    "            return True\n",
    "\n",
    "        for v in range(len(self.matriz[u])):\n",
    "            if self.matriz[u][v] != 0.0 and not visitado[v]:\n",
    "                if self.dfs_util(v, destino, visitado, caminho):\n",
    "                    return True\n",
    "\n",
    "        caminho.pop()\n",
    "        return False\n",
    "\n",
    "    def dfs(self, origem, destino):\n",
    "        visitado = [False] * len(self.matriz)\n",
    "        caminho = []\n",
    "        self.dfs_util(origem, destino, visitado, caminho)\n",
    "        return caminho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_boolean_array(size):\n",
    "    return np.zeros(size) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_true_index(array_index, path):\n",
    "    for i in range(len(path)):\n",
    "        array_index[path[i]] = False\n",
    "    return array_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_sensors_matrix(transition_matrix, array_index):\n",
    "    transition_matrix = transition_matrix[:,array_index]\n",
    "    return transition_matrix[array_index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB3HN48JlLGD"
   },
   "source": [
    "### Code for sensors sparsity testing - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A74VJIUMxRXZ"
   },
   "outputs": [],
   "source": [
    "# Load existent data\n",
    "probability = 0.9\n",
    "name = \"Teste Esparcidade Geográfica - GNN/sensors90.npz\"\n",
    "data_m = np.load(name)\n",
    "array_index = data_m['arr_0']\n",
    "transition_matrix = remove_sensors(transition_matrix, array_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9FTuQbwxRXZ"
   },
   "outputs": [],
   "source": [
    "# Create new random array\n",
    "lin, col = transition_matrix.shape\n",
    "probability = 0.2\n",
    "array_index = random_index(col, probability)\n",
    "transition_matrix = remove_sensors(transition_matrix, array_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcPZtwnZxRXy"
   },
   "outputs": [],
   "source": [
    "array_index_np = np.array(array_index)\n",
    "np.savez(\"Teste Esparcidade Geográfica - Regressão/sensors10.npz\",array_index_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSWz7HAelLGH"
   },
   "source": [
    "### Load speed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ri-hAcL6Z9H"
   },
   "outputs": [],
   "source": [
    "dataf = pd.read_csv('max_speed2_pemsd3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJTWa51H6Z9H"
   },
   "outputs": [],
   "source": [
    "df_avg = pd.read_csv('avg_speed2_pemsd3.csv')\n",
    "list_avg = df_avg['Avg Speed'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMaV9dHf6Z9H"
   },
   "outputs": [],
   "source": [
    "list_max = dataf['Max Speed'].values\n",
    "list_count = dataf['Count'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9P4NgLjL6Z9H"
   },
   "outputs": [],
   "source": [
    "list_max = interpolate_values(list_max, -m.inf)\n",
    "list_avg = interpolate_values(list_avg, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afZlp8-llLGL"
   },
   "source": [
    "### Code for sensors sparsity tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5U005b-lLGM"
   },
   "outputs": [],
   "source": [
    "list_max = remove_sensors_list(list_max, array_index)\n",
    "list_avg = remove_sensors_list(list_avg, array_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joBsFUVBlLGN"
   },
   "source": [
    "### Generate transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K50rc_-u6Z9I",
    "outputId": "54504afb-6ccd-47c7-aef4-e4d99ff15205"
   },
   "outputs": [],
   "source": [
    "matrix = definitive_transition_matrix(transition_matrix, list_max, list_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXvz8DPZ6Z9I",
    "outputId": "739d0f75-935b-4f60-83f8-c49334d56656"
   },
   "outputs": [],
   "source": [
    "matrix_sparse = sp.sparse.csr_matrix(matrix)\n",
    "print(matrix_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAPhwe5a6Z9Q"
   },
   "source": [
    "## Generation of Neural Network input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = np.load(\"list_pemsd3.npy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sP_BS_v6Z9Q"
   },
   "outputs": [],
   "source": [
    "initial_date = dt.datetime(2023, 1, 1, 0, 0, 0)\n",
    "final_date = dt.datetime(2023, 8, 28, 23, 55, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nguj7_vm6Z9Q"
   },
   "outputs": [],
   "source": [
    "list_datetime = list_datetime_timedelta(initial_date,final_date, 5)\n",
    "list_datetime = list_datetime[:(len(list_datetime)//8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolut_nan = pd_nan.iloc[0]['NaN Values']\n",
    "total = len(numpy_array)\n",
    "percent = pd_nan.iloc[0]['Percentage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ST_jJvpx6Z9R"
   },
   "outputs": [],
   "source": [
    "df_matrix = pd.read_csv('matrix2_pemsd3.csv')\n",
    "numpy_array = df_matrix.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_matrix = np.load(\"PEMSd3/data/Noise/Noise 5/noise_matrix.npy\")\n",
    "numpy_array = sum_noise_matrix(numpy_array, noise_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WA4Yi8VDxRX3"
   },
   "source": [
    "### Code for sensors sparsity testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sA9e7TykxRX5"
   },
   "outputs": [],
   "source": [
    "numpy_array = update_data_matrix(numpy_array, array_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDPcU6YhlLGU"
   },
   "source": [
    "### Evaluate interpolated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXCs8xGf6Z9R",
    "outputId": "eb4491c5-63ea-48f9-eaa4-4c39b9f4dfc6"
   },
   "outputs": [],
   "source": [
    "lin, col = numpy_array.shape\n",
    "absolut_nan,total,percent, df_lines = avaliate_nan_values(numpy_array)\n",
    "print(\"Total de NaN values:\",absolut_nan)\n",
    "print(\"Total de entradas da matriz:\",total)\n",
    "print(\"Percentual NaN/Total:\",percent,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNiAGdjlxRX6"
   },
   "source": [
    "### Code for interpolation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SkJt8fkSxRX7"
   },
   "outputs": [],
   "source": [
    "interpolate_rate = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YByHPLwzxRX8"
   },
   "outputs": [],
   "source": [
    "# Load existent matrix\n",
    "name = \"Teste Interpolação - Regressão/matrix_0.npz\"\n",
    "data_m = np.load(name)\n",
    "numpy_array = data_m['array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Z5ZKrAGxRX8"
   },
   "outputs": [],
   "source": [
    "# Create new matrix\n",
    "numpy_array = remove_random_data(numpy_array, interpolate_rate, len(list_datetime))\n",
    "np.savez_compressed('Teste Interpolação - Regressão/matrix_0.npz', array=numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2BmDvM1xRX-",
    "outputId": "922146ad-299f-4188-a0ae-f5180fac42b9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lin, col = numpy_array.shape\n",
    "absolut_nan2,total2,percent2, df_lines = avaliate_nan_values(numpy_array[:,:col//8])\n",
    "print(\"Total de NaN values:\",absolut_nan2)\n",
    "print(\"Total de entradas da matriz:\",total2)\n",
    "print(\"Percentual NaN/Total:\",percent2,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTuzIM1QxRX_"
   },
   "outputs": [],
   "source": [
    "numpy_array = update_data_matrix(numpy_array, array_index)\n",
    "print(numpy_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTa0SBh7xRYA"
   },
   "source": [
    "## Data Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypS-jtDslLGg"
   },
   "outputs": [],
   "source": [
    "data_id_d = 0\n",
    "name_d = 'PEMSd3'\n",
    "initial_date_d = str(initial_date)\n",
    "final_date_d = str(final_date)\n",
    "samples_interval = 5\n",
    "plus_interval = 1\n",
    "short_samples = 2\n",
    "medium_time = 24\n",
    "medium_samples = 6\n",
    "long_time = 168\n",
    "long_samples = 12\n",
    "sensors = 1\n",
    "#sensors = len(df_lines)\n",
    "#interpolate_data = percent\n",
    "interpolate_data = 0\n",
    "observations = \"\"\n",
    "name_archive_data = \"data_metrics.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_CrLnlqxRYG"
   },
   "source": [
    "## Input Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6kJM822mIe0"
   },
   "source": [
    "### Interpolate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_flow = interpolate_list(numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drmf_RQ86Z9S",
    "outputId": "82a0c653-f1ce-4b61-b5c9-8ac53b7eaa1e"
   },
   "outputs": [],
   "source": [
    "#matrix_flow, exclude_lines2 = interpolate_matrix(numpy_array)\n",
    "matrix_flow = spline_interpolate_matrix(numpy_array)\n",
    "matrix_flow = numpy_array[:,:col//8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_flow = numpy_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gI7aJU6Jl9Td"
   },
   "source": [
    "### Code for temporal sparsity testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpnl9pc_xRYH",
    "outputId": "e30ef6ff-77c8-4676-f746-6fb2ef01b1ee"
   },
   "outputs": [],
   "source": [
    "matrix_flow = remove_data(matrix_flow,plus_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdU5LeECxRYH"
   },
   "outputs": [],
   "source": [
    "list_datetime = list_datetime[:matrix_flow.shape[1]]\n",
    "print(list_datetime[0],list_datetime[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kz7FTFRNm_9F"
   },
   "source": [
    "### Generate input data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gdr9ZI4FxRYJ",
    "outputId": "fb2c296c-67c8-4921-d89a-442c7c597329"
   },
   "outputs": [],
   "source": [
    "list_Xt = create_list_Xt(matrix_flow,list_datetime, medium_time, long_time, medium_samples, long_samples)\n",
    "print(len(list_Xt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFvLh53bxRYJ"
   },
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSKCKoEbxRYJ"
   },
   "outputs": [],
   "source": [
    "data_id_m = 0\n",
    "model_id_m = 0\n",
    "split_array = [0.4,0.5]\n",
    "final_relu = False\n",
    "conv_array = [1]\n",
    "k_array = [4]\n",
    "lr = 0.01\n",
    "name_archive_model = \"model_metrics.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_qfGoxz6Z9T"
   },
   "source": [
    "## Separation of Data into Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQhOmibV6Z9T"
   },
   "outputs": [],
   "source": [
    "training, validation, test = sequential_split(list_Xt,split_array[0],split_array[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqtjOXMA6Z9U",
    "outputId": "de71b8f1-50a7-45a6-c0c4-b0e6a7afbb2e"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset_C(matrix_sparse,training)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1iYMwt-6Z9U",
    "outputId": "ca51c99f-53d0-4fac-ab7b-beb11faf3878"
   },
   "outputs": [],
   "source": [
    "dataset_validation = Dataset_C(matrix_sparse,validation)\n",
    "print(dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjGD7LmQ6Z9V",
    "outputId": "1bc18ca6-91f2-4c81-8cb8-ff23c06ec6ee"
   },
   "outputs": [],
   "source": [
    "dataset_test = Dataset_C(matrix_sparse,test)\n",
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kWqQliaxRYO"
   },
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nq3lW5KIxRYO"
   },
   "outputs": [],
   "source": [
    "batch_size_training = 1\n",
    "epochs_training = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lwkOE2Y6Z9V"
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, min_lr=0.00001)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JniR9zeH6Z9W"
   },
   "outputs": [],
   "source": [
    "# Create and compile model\n",
    "model = GNN(conv_array, k_array, final_relu)\n",
    "model.compile(optimizer=optimizer,loss=squared_error, metrics=[rmse,nrmse,\"mae\",\"mape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bqUrlCl6Z9W"
   },
   "outputs": [],
   "source": [
    "# Create Loaders\n",
    "loader = BatchLoader(dataset, batch_size=batch_size_training,shuffle=True)\n",
    "loader_validation = BatchLoader(dataset_validation, batch_size=batch_size_training)\n",
    "loader_test = BatchLoader(dataset_test, batch_size=batch_size_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDUcfS6TxRYQ",
    "outputId": "f1a87a2a-b0d7-47be-da3b-4e918363482d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "init_time = time.time()\n",
    "metrics_fit = model.fit(loader.load(), use_multiprocessing=True, workers=-1, verbose=0, steps_per_epoch=loader.steps_per_epoch, epochs=epochs_training, validation_data=loader_validation.load(), validation_steps=loader_validation.steps_per_epoch, callbacks=[reduce_lr, tensorboard_callback])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFc0qpP_6Z9a",
    "outputId": "b6589a10-b301-4584-802e-4f83842b6b23"
   },
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "init_time = time.time()\n",
    "metrics_validation = model.evaluate(loader_validation.load(), steps=loader_validation.steps_per_epoch)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--5HL_xP6Z9a",
    "outputId": "8dc5df41-fb6e-41f4-85b1-012bd0577a20"
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "init_time = time.time()\n",
    "metrics_testing = model.evaluate(loader_test.load(), steps=loader_test.steps_per_epoch)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvxBlFpS6Z9a"
   },
   "source": [
    "## Data Separation for Linear Regression Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpl6B9FBxRYY"
   },
   "outputs": [],
   "source": [
    "def data_split_training(list_Xt,split_array):\n",
    "    training, validation, test = sequential_split(list_Xt,split_array[0],split_array[1])\n",
    "    x_training, y_training = split_x_and_y(training)\n",
    "    return (x_training, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCRvYyiwxRYY"
   },
   "outputs": [],
   "source": [
    "def data_split_validation(list_Xt,split_array):\n",
    "    training, validation, test = sequential_split(list_Xt,split_array[0],split_array[1])\n",
    "    x_validation, y_validation = split_x_and_y(validation)\n",
    "    return (x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBaqe_iExRYZ"
   },
   "outputs": [],
   "source": [
    "def data_split_test(list_Xt,split_array):\n",
    "    training, validation, test = sequential_split(list_Xt,split_array[0],split_array[1])\n",
    "    x_test, y_test = split_x_and_y(test)\n",
    "    return (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geAVlTVsxRYZ"
   },
   "outputs": [],
   "source": [
    "def data_training(matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array):\n",
    "    list_Xt = create_list_Xt(matrix_flow,list_datetime, medium_time, long_time, medium_samples, long_samples)\n",
    "    return data_split_training(list_Xt,split_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WyPN8cTxRYZ"
   },
   "outputs": [],
   "source": [
    "def data_validation(matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array):\n",
    "    list_Xt = create_list_Xt(matrix_flow,list_datetime, medium_time, long_time, medium_samples, long_samples)\n",
    "    return  data_split_validation(list_Xt,split_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JjhaxkSxRYa"
   },
   "outputs": [],
   "source": [
    "def data_test(matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array):\n",
    "    list_Xt = create_list_Xt(matrix_flow,list_datetime, medium_time, long_time, medium_samples, long_samples)\n",
    "    return  data_split_test(list_Xt,split_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension(matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array):\n",
    "    x_validation, y_validation = data_validation(matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "    return x_validation[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blEVFmmGxRYb"
   },
   "outputs": [],
   "source": [
    "x_training, y_training = data_training(matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "x_validation, y_validation = data_validation(matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "x_test, y_test = data_test(matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyZWivF1xRYc"
   },
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkPQ7T7WxRYc"
   },
   "outputs": [],
   "source": [
    "batch_size_training = 300\n",
    "epochs_training = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression and Multiple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(lin, col):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(col)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse',metrics=[rmse,nrmse,\"mae\",\"mape\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(lin, col):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=(col)),              \n",
    "        tf.keras.layers.Dense(128, activation='relu'),  \n",
    "        tf.keras.layers.Dropout(0.2),                   \n",
    "        tf.keras.layers.Dense(1)                        \n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse',metrics=[rmse,nrmse,\"mae\",\"mape\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_regressions(lines, columns):\n",
    "    listt = []\n",
    "    for i in range(lines):\n",
    "        listt.append(linear_regression(1, columns))\n",
    "    return listt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_fully_connected(lines, columns):\n",
    "    listt = []\n",
    "    for i in range(lines):\n",
    "        listt.append(fully_connected(1, columns))\n",
    "    return listt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_model(listt, path):\n",
    "    for i in range(len(listt)):\n",
    "        name = path + f\"/model_{i}.h5\"\n",
    "        listt[i].load_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(listt, temporal_series, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array, path):\n",
    "    batch_size_training = 1\n",
    "    epochs_training = 50\n",
    "    for i in range(145, len(listt)):\n",
    "        print(i)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, min_lr=0.00001)\n",
    "        x_training, y_training = data_training(temporal_series[i], list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "        listt[i].fit(x_training, y_training, epochs=epochs_training, batch_size=batch_size_training, verbose=0, callbacks=[reduce_lr])\n",
    "        path2 = path + f\"/model_{i}.h5\"\n",
    "        listt[i].save_weights(path2)\n",
    "        del x_training\n",
    "        del y_training\n",
    "        del reduce_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_index_model(listt, temporal_series, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array, index, path):\n",
    "    batch_size_training = 1\n",
    "    epochs_training = 50\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, min_lr=0.00001)\n",
    "    x_training, y_training = data_training(temporal_series[index], list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "    listt[index].fit(x_training, y_training, epochs=epochs_training, batch_size=batch_size_training, verbose=1, callbacks=[reduce_lr])\n",
    "    path2 = path + f\"/model_{index}.h5\"\n",
    "    listt[index].save_weights(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_train_model(listt, temporal_series, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array):\n",
    "    for i in range(len(listt)):\n",
    "        print(i)\n",
    "        x_training, y_training = data_training(temporal_series[i], list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "        listt[i].evaluate(x_training, y_training)\n",
    "        del x_training\n",
    "        del y_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_training_model(listt, temporal_series, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array):\n",
    "    x_training, y_training_og = data_training(temporal_series, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "    del x_training\n",
    "    print(y_training_og, y_training_og.shape)\n",
    "    prev_values = np.zeros((len(y_training_og), len(listt)),dtype=float)\n",
    "    print(len(y_training_og), len(listt))\n",
    "    for i in range(len(listt)):\n",
    "        x_training, y_training = data_training(temporal_series[i], list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "        previsoes = listt[i].predict(x_training)\n",
    "        previsoes = previsoes.flatten()\n",
    "        prev_values[:,i] = previsoes\n",
    "        del x_training\n",
    "        del y_training\n",
    "    new_expected = np.reshape(y_training_og, (*y_training_og.shape, 1))\n",
    "    new_prev = np.reshape(prev_values, (*prev_values.shape, 1))\n",
    "    return (new_expected, new_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_validation_model(listt, temporal_series, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array):\n",
    "    x_training, y_training_og = data_validation(temporal_series, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "    del x_training\n",
    "    print(y_training_og, y_training_og.shape)\n",
    "    prev_values = np.zeros((len(y_training_og), len(listt)),dtype=float)\n",
    "    print(len(y_training_og), len(listt))\n",
    "    for i in range(len(listt)):\n",
    "        x_training, y_training = data_validation(temporal_series[i], list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "        previsoes = listt[i].predict(x_training)\n",
    "        previsoes = previsoes.flatten()\n",
    "        prev_values[:,i] = previsoes\n",
    "        del x_training\n",
    "        del y_training\n",
    "    new_expected = np.reshape(y_training_og, (*y_training_og.shape, 1))\n",
    "    new_prev = np.reshape(prev_values, (*prev_values.shape, 1))\n",
    "    return (new_expected, new_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_model(listt, temporal_series, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array):\n",
    "    x_training, y_training_og = data_test(temporal_series, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "    del x_training\n",
    "    print(y_training_og, y_training_og.shape)\n",
    "    prev_values = np.zeros((len(y_training_og), len(listt)),dtype=float)\n",
    "    print(len(y_training_og), len(listt))\n",
    "    for i in range(len(listt)):\n",
    "        x_training, y_training = data_test(temporal_series[i], list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "        previsoes = listt[i].predict(x_training)\n",
    "        previsoes = previsoes.flatten()\n",
    "        prev_values[:,i] = previsoes\n",
    "        del x_training\n",
    "        del y_training\n",
    "    new_expected = np.reshape(y_training_og, (*y_training_og.shape, 1))\n",
    "    new_prev = np.reshape(prev_values, (*prev_values.shape, 1))\n",
    "    return (new_expected, new_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_training_model(listt, temporal_series, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array):\n",
    "    x_training, y_training_og = data_training(temporal_series, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "    del x_training\n",
    "    expected_values = np.zeros((len(listt), len(y_training_og)),dtype=float)\n",
    "    prev_values = np.zeros((len(listt), len(y_training_og)),dtype=float)\n",
    "    for i in range(len(listt)):\n",
    "        x_training, y_training = data_training(temporal_series[i], list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "        previsoes = listt[i].predict(x_training)\n",
    "        previsoes = previsoes.flatten()\n",
    "        prev_values[i] = previsoes\n",
    "        expected_values[:, i] = y_training_og[i]\n",
    "        del x_training\n",
    "        del y_training\n",
    "    new_expected = np.reshape(expected_values, (*expected_values.shape, 1))\n",
    "    new_prev = np.reshape(prev_values, (*prev_values.shape, 1))\n",
    "    return (new_expected, new_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin, col = dimension(matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "listt_regression = list_regressions(lin, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(listt_cnn, matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array, 'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_model(listt_regression, 'Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expected, prev = predict_training_model(listt_regression, matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expected, prev = predict_validation_model(listt_regression, matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expected, prev = predict_test_model(listt_regression, matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin, col = dimension(matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)\n",
    "listt_cnn = list_fully_connected(lin, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(listt_regression, matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_model(listt_cnn, 'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expected, prev = predict_training_model(listt_cnn, matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expected, prev = predict_validation_model(listt_cnn, matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expected, prev = predict_test_model(listt_cnn, matrix_flow, list_datetime, medium_time, long_time, medium_samples, long_samples, split_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected2 = np.squeeze(expected, axis=-1)\n",
    "prev2 = np.squeeze(prev, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected3 = expected2.T\n",
    "prev3 = prev2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_training = rmse2(expected, prev)\n",
    "nrmse_training = nrmse2(expected, prev)\n",
    "mae_training = mean_absolute_error(expected2, prev2)\n",
    "mape_training = mean_absolute_percentage_error(expected2, prev2)\n",
    "print(\"RMSE:\", rmse_training)\n",
    "print(\"NRMSE:\", nrmse_training)\n",
    "print(\"MAE:\", mae_training)\n",
    "print(\"MAPE:\", mape_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_validation = rmse2(expected, prev)\n",
    "nrmse_validation = nrmse2(expected, prev)\n",
    "mae_validation = mean_absolute_error(expected2, prev2)\n",
    "mape_validation = mean_absolute_percentage_error(expected2, prev2)\n",
    "print(\"RMSE:\", rmse_validation)\n",
    "print(\"NRMSE:\", nrmse_validation)\n",
    "print(\"MAE:\", mae_validation)\n",
    "print(\"MAPE:\", mape_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_testing = rmse2(expected, prev)\n",
    "nrmse_testing = nrmse2(expected, prev)\n",
    "mae_testing = mean_absolute_error(expected2, prev2)\n",
    "mape_testing = mean_absolute_percentage_error(expected2, prev2)\n",
    "print(\"RMSE:\", rmse_test)\n",
    "print(\"NRMSE:\", nrmse_test)\n",
    "print(\"MAE:\", mae_test)\n",
    "print(\"MAPE:\", mape_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_value = rmse2(expected2, prev2)\n",
    "nrmse_value = nrmse2(expected2, prev2)\n",
    "mae_value = mean_absolute_error(expected2, prev2)\n",
    "mape_value = mean_absolute_percentage_error(expected2, prev2)\n",
    "print(\"RMSE:\", rmse_value)\n",
    "print(\"NRMSE:\", nrmse_value)\n",
    "print(\"MAE:\", mae_value)\n",
    "print(\"MAPE:\", mape_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_value = rmse2(expected3, prev3)\n",
    "nrmse_value = nrmse2(expected3, prev3)\n",
    "mae_value = mean_absolute_error(expected3, prev3)\n",
    "mape_value = mean_absolute_percentage_error(expected3, prev3)\n",
    "print(\"RMSE:\", rmse_value)\n",
    "print(\"NRMSE:\", nrmse_value)\n",
    "print(\"MAE:\", mae_value)\n",
    "print(\"MAPE:\", mape_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Fully Connected Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin, col = x_training[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(lin,col)),            \n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),                  \n",
    "    tf.keras.layers.Dense(1)                       \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse',metrics=[rmse,nrmse,\"mae\",\"mape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "metrics_fit = model.fit(x_training, y_training, epochs=epochs_training, batch_size=batch_size_training, validation_data=(x_validation, y_validation), verbose=0, callbacks=[reduce_lr,tensorboard_callback])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "metrics_validation = model.evaluate(x_validation,y_validation)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "metrics_testing = model.evaluate(x_test,y_test)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoGyHL8i6Z9b"
   },
   "source": [
    "## Linear Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKd1ekaQ6Z9b"
   },
   "outputs": [],
   "source": [
    "lin, col = x_training[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qytDgFA6Z9c"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(lin, col)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7naS2n7m6Z9c"
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaYnicL36Z9d"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse',metrics=[rmse,nrmse,\"mae\",\"mape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ojheMi2xRYe"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah8IU0-P6Z9d",
    "outputId": "f98ed266-dc8f-4541-bf94-4010097b0c84"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnayzVNR6Z9d",
    "outputId": "66c82d78-b1d4-43e8-e2c2-e1101b68e9a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "init_time = time.time()\n",
    "metrics_fit = model.fit(x_training, y_training, epochs=epochs_training, batch_size=batch_size_training, validation_data=(x_validation, y_validation), verbose=0, callbacks=[reduce_lr,tensorboard_callback])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrqOAWLm6Z9d",
    "outputId": "c9918559-2d3f-424c-ed0a-4f02656b5e46"
   },
   "outputs": [],
   "source": [
    "# Validates the model\n",
    "init_time = time.time()\n",
    "metrics_validation = model.evaluate(x_validation,y_validation)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAg7MOFJ6Z9d",
    "outputId": "7537acd3-2914-4c5d-f7da-8baa91fb61de"
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "init_time = time.time()\n",
    "metrics_testing = model.evaluate(x_test, y_test)\n",
    "end_time = time.time()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
